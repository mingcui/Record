Project: hybris cluster delpoy
Environment:
    node1: VT-1 (10.202.16.208 centos6.7 hybris5.7)
    node2: VT-2 (10.202.16.226 centos6.7 hybris5.7)
configure:
    1 ) on node1
      add below to local.properties ==>
  vi /app/hybris/hybris/config/local.properties
#for cluster
clustermode=true
cluster.id=0
cluster.maxid=3
cluster.broadcast.methods=jgroups
cluster.broadcast.method.jgroups=de.hybris.platform.cluster.jgroups.JGroupsBroadcastMethod
cluster.broadcast.method.jgroups.tcp.bind_addr=10.202.16.208
cluster.broadcast.method.jgroups.tcp.bind_port=7800
cluster.broadcast.method.jgroups.channel.name=hybris-broadcast
cluster.broadcast.method.jgroups.configuration=jgroups-tcp.xml


    2 ) on node2
     vi /app/hybris/hybris/config/local.properties
#for cluster
clustermode=true
cluster.id=1
cluster.maxid=3
cluster.broadcast.methods=jgroups  
cluster.broadcast.method.jgroups=de.hybris.platform.cluster.jgroups.JGroupsBroadcastMethod
cluster.broadcast.method.jgroups.tcp.bind_addr=10.202.16.226
cluster.broadcast.method.jgroups.tcp.bind_port=7800
cluster.broadcast.method.jgroups.channel.name=hybris-broadcast
cluster.broadcast.method.jgroups.configuration=jgroups-tcp.xml

   3 ) jgroup tcp configure:
  vim ../platform/jgroups-udp.xml
<!-- TCP based stack, with flow control and message bundling. This is usually used when IP multicasting cannot be used in a network,
         e.g. because it is disabled (routers discard multicast).  -->
<config xmlns="urn:org:jgroups" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:org:jgroups http://www.jgroups.org/schema/JGroups-3.0.xsd">
    <TCP loopback="true"
        recv_buf_size="${tcp.recv_buf_size:20M}"
        send_buf_size="${tcp.send_buf_size:640K}"
        discard_incompatible_packets="true"
        max_bundle_size="64K"
        max_bundle_timeout="30"
        enable_bundling="true"
        use_send_queues="true"
        sock_conn_timeout="300"
        timer_type="new"
        timer.min_threads="4"
        timer.max_threads="10"
        timer.keep_alive_time="3000"
        timer.queue_max_size="500"
        thread_pool.enabled="true"
        thread_pool.min_threads="1"
        thread_pool.max_threads="10"
        thread_pool.keep_alive_time="5000"
        thread_pool.queue_enabled="false"
        thread_pool.queue_max_size="100"
        thread_pool.rejection_policy="discard"
        oob_thread_pool.enabled="true"
        oob_thread_pool.min_threads="1"
        oob_thread_pool.max_threads="8"
        oob_thread_pool.keep_alive_time="5000"
        oob_thread_pool.queue_enabled="false"
        oob_thread_pool.queue_max_size="100"
        oob_thread_pool.rejection_policy="discard"
        bind_addr="${hybris.jgroups.bind_addr}"
        bind_port="${hybris.jgroups.bind_port}" />

    <JDBC_PING connection_driver="${hybris.database.driver}"
        connection_password="${hybris.database.password}"
        connection_username="${hybris.database.user}"
        connection_url="${hybris.database.url}"
        initialize_sql="${hybris.jgroups.schema}"/>

    <MERGE2 min_interval="10000" max_interval="30000" />
    <FD_SOCK />
    <FD timeout="3000" max_tries="3" />
    <VERIFY_SUSPECT timeout="1500" />
    <BARRIER />
    <pbcast.NAKACK use_mcast_xmit="true" exponential_backoff="500" discard_delivered_msgs="true" />
    <UNICAST />
    <pbcast.STABLE stability_delay="1000" desired_avg_gossip="50000" max_bytes="4M" />
    <pbcast.GMS print_local_addr="true" join_timeout="3000" view_bundling="true" />
    <UFC max_credits="2M" min_threshold="0.4" />
    <MFC max_credits="2M" min_threshold="0.4" />
    <FRAG2 frag_size="60K" />
    <pbcast.STATE_TRANSFER />
</config>


   4 ) check:
access hac(9001):
user:admin
passwd: nimda
   5 ) configure session tomcat session replication
     5-1) enable member ship
     in both node vi /app/hybirs/hybris/bin/platform/tomcat/conf/server.xml, add bleow in <Engine name="Catalina"...>...</Engine>
<Cluster className="org.apache.catalina.ha.tcp.SimpleTcpCluster"
       channelSendOptions="8">
 
   <Manager className="org.apache.catalina.ha.session.DeltaManager"
          expireSessionsOnShutdown="false"
          notifyListenersOnReplication="true"/>
 
   <Channel className="org.apache.catalina.tribes.group.GroupChannel">
      <Membership className="org.apache.catalina.tribes.membership.McastService"
               address="228.0.0.4"       
               port="45564"
               frequency="500"
               dropTime="3000"/>
      <Receiver className="org.apache.catalina.tribes.transport.nio.NioReceiver"
              address="auto"
              port="4000"
              autoBind="100"
              selectorTimeout="5000"
              maxThreads="6"/>
 
      <Sender className="org.apache.catalina.tribes.transport.ReplicationTransmitter">
         <Transport className="org.apache.catalina.tribes.transport.nio.PooledParallelSender"/>
      </Sender>
      <Interceptor className="org.apache.catalina.tribes.group.interceptors.TcpFailureDetector"/>
      <Interceptor className="org.apache.catalina.tribes.group.interceptors.MessageDispatch15Interceptor"/>
   </Channel>
 
   <Valve className="org.apache.catalina.ha.tcp.ReplicationValve" filter=""/>
   <Valve className="org.apache.catalina.ha.session.JvmRouteBinderValve"/>
 
 
   <ClusterListener className="org.apache.catalina.ha.session.JvmRouteSessionIDBinderListener"/>
   <ClusterListener className="org.apache.catalina.ha.session.ClusterSessionListener"/>
</Cluster>

     5-2) configure session manager for all extensions
    vi server.xml replace ->
<!-- 'yacceleratorstorefront' extension's context for tenant 'master' -->
<Manager pathname="" />                              to
<Manager className="org.apache.catalina.ha.session.DeltaManager" expireSessionsOnShutdown="false" notifyListenersOnReplication="true" />

     #ant all
